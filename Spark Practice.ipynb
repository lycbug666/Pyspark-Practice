{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Programming Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Basic Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "-53\n",
      "PythonRDD[97] at RDD at PythonRDD.scala:48\n",
      "55\n",
      "15\n",
      "55\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(range(1,11),1)\n",
    "print (x.reduce(lambda x,y : x+y ))\n",
    "print (x.reduce(lambda x,y : x-y ))\n",
    "# 1+2+...+10=55\n",
    "# 1-2-3-4-...-10=-53\n",
    "x = sc.parallelize(range(1,11),2)\n",
    "print (x)\n",
    "print (x.reduce(lambda x,y : x+y ))\n",
    "print (x.reduce(lambda x,y : x-y ))\n",
    "# 1+2+3+4+5 + 6+7+8+9+10 = 55\n",
    "# 1-2-3-4-5 - 6-7-8-9-10 = -13 - -28 = 15\n",
    "\n",
    "x = sc.parallelize(range(1,11),3)\n",
    "print (x.reduce(lambda x,y : x+y ))\n",
    "print (x.reduce(lambda x,y : x-y ))\n",
    "# 1-4-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.takeOrdered(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "x.top(3)\n",
    "\n",
    "for num in x.top(3):\n",
    "    print (num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'a': 2, 'b': 2})\n",
      "a\n",
      "b\n",
      "{'a': 3, 'b': 4}\n",
      "a\n",
      "b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\",1), (\"b\", 2), (\"a\", 3), (\"b\", 4)], 2)\n",
    "print (rdd.countByKey())\n",
    "\n",
    "for num in rdd.countByKey():\n",
    "    print (num)\n",
    "\n",
    "print (rdd.collectAsMap())\n",
    "\n",
    "for num in rdd.collectAsMap():\n",
    "    print (num)\n",
    "\n",
    "rdd.lookup(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "10\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "# Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral “zero value.”\n",
    "x = sc.parallelize([1,2,3,4])\n",
    "neutral_zero_value = 0  # 0 for sum, 1 for multiplication\n",
    "y = x.fold(neutral_zero_value,lambda obj, accumulated: accumulated + obj) # computes cumulative sum\n",
    "print(x.collect())\n",
    "print(y)\n",
    "\n",
    "x.fold(1, lambda x,y : x*y)\n",
    "\n",
    "def f(a,b):\n",
    "    if a> b :\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "y = x.fold(neutral_zero_value,lambda obj, accumulated: f(obj,accumulated)) # computes cumulative sum\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "(9, 24)\n",
      "(10, 4)\n",
      "(14, 8)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate  聚合每个partition的元素，然后再聚合每个partition\n",
    "x = sc.parallelize([2,3,4])\n",
    "neutral_zero_value = (0,1) # sum: x+0 = x, product: 1*x = x\n",
    "seqOp = (lambda aggregated, el: (aggregated[0] + el, aggregated[1] * el)) \n",
    "combOp = (lambda aggregated, el: (aggregated[0] + el[0], aggregated[1] * el[1]))\n",
    "y = x.aggregate(neutral_zero_value,seqOp,combOp)  # computes (cumulative sum, cumulative product)\n",
    "print(x.collect())\n",
    "print(y)\n",
    "\n",
    "x = sc.parallelize([1,2,3,4], 2)\n",
    "y = x.aggregate((0,0), (lambda acc, value: (acc[0]+value, acc[1]+1)), (lambda acc1,acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])))\n",
    "print (y)\n",
    "x = sc.parallelize([1,2,3,4],3)\n",
    "y= x.aggregate((1,1), (lambda acc, value: (acc[0]+value, acc[1]+1)), (lambda acc1,acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])))\n",
    "print (y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "0 \n",
      "1 \n",
      "0 \n",
      "1 \n",
      "2 \n",
      "0 \n",
      "1 \n",
      "2 \n",
      "3 \n",
      "range(0, 1) \n",
      "range(0, 2) \n",
      "range(0, 3) \n",
      "range(0, 4) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 9]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4],2)\n",
    "y = x.flatMap(lambda x: (range(x))).collect()\n",
    "for num in y:\n",
    "    print (str(num) + \" \")\n",
    "\n",
    "y = x.map(lambda x: (range(x))).collect()\n",
    "for num in y:\n",
    "    print (str(num) + \" \")\n",
    "    \n",
    "\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5], 4)\n",
    "def f(iterator): \n",
    "    yield sum(iterator)\n",
    "rdd.mapPartitions(f).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "rdd.mapPartitionsWithIndex(f).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 3, 3, 3, 8, 10, 10, 13, 14, 17, 20, 20, 22, 22, 22, 25, 27, 28, 30, 36, 38, 41, 47, 48, 53, 53, 55, 57, 59, 62, 63, 71, 76, 78, 78, 78, 79, 80, 80, 80, 80, 83, 83, 84, 84, 85, 85, 86, 88, 90, 90, 91, 92, 92]\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "x = sc.parallelize((range(100)))\n",
    "sample = x.sample(withReplacement=True, fraction=0.5).collect()\n",
    "print (sample)\n",
    "\n",
    "# takeSample 和sample同发相似，但是第二个参数换了变量，返回的是Collection并非RDD.\n",
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  # call 'sample' 5 times\n",
    "print('x = ' + str(x.collect()))\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coalesce\n",
    "sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(2).collect()\n",
    "sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(2).glom().collect()\n",
    "\n",
    "#glom\n",
    "sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random split\n",
    "rdd = sc.parallelize(range(500), 1)\n",
    "rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
    "len(rdd1.collect() + rdd2.collect())\n",
    "#500\n",
    "150 < rdd1.count() < 250\n",
    "#true\n",
    "250 < rdd2.count() < 350\n",
    "\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n",
      "[[1, 2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "# repartition 可以在最初对RDD进行分区操作，这个操作实际上是一个shuffle，可能比较耗时，但是如果之后的action比较多的话，\n",
    "# 可以减少以后的操作时间，其中N值看这个cpu的个数，一般大于2倍cpu，小于1000\n",
    "\n",
    "# coalesce \n",
    "# 返回一个新的RDD，经过reduce操作后到一个numPartitions 的分区当中,可能需要shuffle\n",
    "# you go from 1000 partitions to 100 partitions, there will not be a shuffle, \n",
    "# instead each of the 100 new partitions will claim 10 of the current partitions.\n",
    "\n",
    "print (sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect())\n",
    "print (sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect())\n",
    "\n",
    "# glom() 返回一个RDD 用于分组一个RDD到一个list\n",
    "# glom() flattens elements on the same partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B', 'D', 'C', 'A']\n",
      "['A']\n",
      "[('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D')]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['D','C','A'])\n",
    "# union 并集\n",
    "z = x.union(y)\n",
    "print(z.collect())\n",
    "\n",
    "# intersection 交集\n",
    "z = x.intersection(y)\n",
    "print(z.collect())\n",
    "\n",
    "# cartesian 笛卡尔积\n",
    "x = sc.parallelize(['A','B'])\n",
    "y = sc.parallelize(['C','D'])\n",
    "z = x.cartesian(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-v transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
      "[('A', [1, 4, 9]), ('B', [16, 25])]\n"
     ]
    }
   ],
   "source": [
    "#mapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.mapValues(lambda x : [i**2 for i in x])\n",
    "print (x.collect())\n",
    "print (y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
      "[('A', 1), ('A', 4), ('A', 9), ('B', 16), ('B', 25)]\n",
      "[('A', (1, 2, 3)), ('A', 'x'), ('B', (4, 5)), ('B', 'x')]\n"
     ]
    }
   ],
   "source": [
    "#flatMapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.flatMapValues(lambda x : [i**2 for i in x])\n",
    "print (x.collect())\n",
    "print (y.collect())\n",
    "y = x.flatMapValues(lambda x : list([x,\"x\"]))\n",
    "print (y.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5), ('A', 5), ('c', 6)]\n",
      "[('B', [(1, 1), (2, 4)]), ('c', [(6, 36)]), ('A', [(3, 9), (4, 16), (5, 25), (5, 25)])]\n"
     ]
    }
   ],
   "source": [
    "# combineByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5), ('A', 5), ('C',6)])\n",
    "createCombiner = (lambda el: [(el,el**2)]) \n",
    "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)]) # append to aggregated\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )  # append agg1 with agg2 partition之间\n",
    "y = x.combineByKey(createCombiner,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "print(y.collect())j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5), ('A', 5)]\n",
      "[('B', (1, 1, 2, 4)), ('A', (3, 9, 4, 16, 5, 25, 5, 25))]\n"
     ]
    }
   ],
   "source": [
    "# combineByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5), ('A', 5)])\n",
    "createCombiner = (lambda el: (el,el**2)) \n",
    "mergeVal = (lambda aggregated, el: aggregated + (el,el**2)) # append to aggregated\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )  # append agg1 with agg2 partition之间\n",
    "y = x.combineByKey(createCombiner,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1.5), ('A', 4.0)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "createCombiner = (lambda x: (x,1))\n",
    "mergeVal = (lambda aggregated , el : (aggregated[0]+el, aggregated[1]+1))\n",
    "mergeComb = (lambda agg1, agg2: (agg1[0]+agg2[0],agg1[1]+agg2[1]))\n",
    "y = x.combineByKey(createCombiner, mergeVal, mergeComb).mapValues(lambda x: x[0]/x[1])\n",
    "print (y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遍历RDD[(K,V)]中每一个元素\n",
    "\n",
    "1、如果当前K是一个新元素，使用createCombiner()创建K为键的累加器初始值，生成列表[('A',(3,9))], [('B',(1,1))]\n",
    "\n",
    "2、如果当前K已经遇到过，使用mergeValue()将当前(K,V)合并进第1步生成的累加器列表,，生成[('A',(3,9)),('A',(4,16)),('A',(5,25)),('B',(1,1,)), ('B', (2,4))],否则执行第1步\n",
    "\n",
    "3、将相同键值的累加器进行合并，得到[('A', [(3, 9), (4, 16), (5, 25)]), ('B', [(1, 1), (2, 4)])]\n",
    "\n",
    "因此得到如下结果：\n",
    "\n",
    "[('A', [(3, 9), (4, 16), (5, 25)]), ('B', [(1, 1), (2, 4)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5), ('A', 5)]\n",
      "[('B', [1, 2]), ('A', [3, 4, 5, 5])]\n"
     ]
    }
   ],
   "source": [
    "# combineByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5), ('A', 5)], 3)\n",
    "createCombiner = (lambda el: [el]) \n",
    "mergeVal = (lambda aggregated, el: aggregated + [el]) # append to aggregated\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )  # append agg1 with agg2 partition之间\n",
    "y = x.combineByKey(createCombiner,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2, 2]), ('b', [1])]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2),(\"a\", 2)])\n",
    "def to_list(a):\n",
    "    return [a]\n",
    "\n",
    "def append(a, b):\n",
    "    a.append(b)\n",
    "    return a\n",
    "    \n",
    "def extend(a, b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "sorted(x.combineByKey(to_list, append, extend).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5), ('b', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# foldByKey\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2),(\"a\", 2)])\n",
    "sorted(x.foldByKey(0, (lambda x,y : x+y)).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', 3), ('A', 12)]\n"
     ]
    }
   ],
   "source": [
    "# ReduceBykey\n",
    "#针对KV结构的数据将相同的key的values放到一起，与groupByKey不同，会进行一个类似mapreduce中的combine操作，减少相应数据的IO操作，加快效率. \n",
    "# 如果想进行一些非叠加操作，我们可以将value组合成字符串或其他格式将相同key的values组合在一起，通过迭代组合数据拆开操作.\n",
    "\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', <pyspark.resultiterable.ResultIterable object at 0x7f7e3a2abeb8>), ('A', <pyspark.resultiterable.ResultIterable object at 0x7f7e3a32c710>)]\n",
      "['B', <pyspark.resultiterable.ResultIterable object at 0x7f7e3a32c978>]\n",
      "['A', <pyspark.resultiterable.ResultIterable object at 0x7f7e3a32c828>]\n",
      "B <pyspark.resultiterable.ResultIterable object at 0x7f7e3a32c160>\n",
      "A <pyspark.resultiterable.ResultIterable object at 0x7f7e3a32c6a0>\n"
     ]
    }
   ],
   "source": [
    "# groupByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.groupByKey()\n",
    "print (y.collect())\n",
    "\n",
    "for num in y.collect():\n",
    "    print (list(num))\n",
    "\n",
    "for key,val in list(y.collect()):\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "\n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.sortByKey(False)\n",
    "print (y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3),\n",
       " ('fleece', 7),\n",
       " ('had', 2),\n",
       " ('lamb', 5),\n",
       " ('little', 4),\n",
       " ('Mary', 1),\n",
       " ('was', 8),\n",
       " ('white', 9),\n",
       " ('whose', 6)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', (3, 3)), ('A', 2), ('A', (1, 1))]\n",
      "[('B', (7, 7)), ('A', 6), ('D', (5, 5))]\n",
      "B [[(3, 3)], [(7, 7)]]\n",
      "D [[], [(5, 5)]]\n",
      "C [[4], []]\n",
      "A [[2, (1, 1)], [6]]\n"
     ]
    }
   ],
   "source": [
    "# groupWith\n",
    "# 在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。\n",
    "# 这个操作在其它框架上称为CoGroup\n",
    "\n",
    "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
    "y = sc.parallelize([('B',(7,7)),('A',6),('D',(5,5))])\n",
    "#z = sc.parallelize([('D',9),('B',(8,8))])\n",
    "#a = x.groupWith(y,z)\n",
    "a = x.groupWith(y)\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "for key,val in list(a.collect()):\n",
    "    print(key, [list(i) for i in val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', (3, 3)), ('A', 2), ('A', (1, 1))]\n",
      "[('A', 8), ('B', 7), ('A', 6), ('D', (5, 5))]\n",
      "B [[(3, 3)], [7]]\n",
      "D [[], [(5, 5)]]\n",
      "C [[4], []]\n",
      "A [[2, (1, 1)], [8, 6]]\n"
     ]
    }
   ],
   "source": [
    "# cogroup\n",
    "# 对两个RDD中的KV元素，每个RDD中相同key中的元素分别聚合成一个集合，与reduceByKey不同的是cogroup针对两个RDD中相同的key元素进行合并.\n",
    "# 不同RDD当中的KV元素分别 放在不同的集合当中.\n",
    "\n",
    "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',(5,5))])\n",
    "z = x.cogroup(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "for key,val in list(z.collect()):\n",
    "    # print(key, [list(i) for i in val])\n",
    "    print(key, [list(i) for i in val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 2), ('b', 3), ('c', 1)]\n",
      "[('a', 3), ('a', 4), ('b', 5)]\n",
      "b [[3], [5]]\n",
      "c [[1], []]\n",
      "a [[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "# cogroup\n",
    "x = sc.parallelize([('a',1),('a',2),('b',3), ('c',1)])\n",
    "y = sc.parallelize([('a',3),('a',4),('b',5)])\n",
    "\n",
    "z = x.cogroup(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "for key,val in z.collect():\n",
    "    # print(key, [list(i) for i in val])\n",
    "    print(key, [list(i) for i in val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 2), ('b', 3), ('c', 1)]\n",
      "[('a', 3), ('a', 4), ('b', 5)]\n",
      "b [[3], [5]]\n",
      "c [[1], []]\n",
      "a [[1, 2], [3, 4]]\n",
      "[('b', (3, 5)), ('a', (1, 3)), ('a', (1, 4)), ('a', (2, 3)), ('a', (2, 4))]\n"
     ]
    }
   ],
   "source": [
    "# join(otherDataSet,numPartitions):对两个RDD先进行cogroup操作形成新的RDD，再对每个Key下的元素进行笛卡尔积\n",
    "x = sc.parallelize([('a',1),('a',2),('b',3), ('c',1)])\n",
    "y = sc.parallelize([('a',3),('a',4),('b',5)])\n",
    "\n",
    "z = x.cogroup(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "for key,val in z.collect():\n",
    "    # print(key, [list(i) for i in val])\n",
    "    print(key, [list(i) for i in val])\n",
    "\n",
    "z = x.join(y)\n",
    "print (z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (3, 5)), ('c', (1, None)), ('a', (1, 3)), ('a', (1, 4)), ('a', (2, 3)), ('a', (2, 4))]\n",
      "[('b', (3, 5)), ('c', (None, 1)), ('a', (1, 3)), ('a', (1, 4)), ('a', (2, 3)), ('a', (2, 4))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('a',1),('a',2),('b',3), ('c',1)])\n",
    "y = sc.parallelize([('a',3),('a',4),('b',5)])\n",
    "\n",
    "z = x.leftOuterJoin(y)\n",
    "print (z.collect())\n",
    "\n",
    "x = sc.parallelize([('a',1),('a',2),('b',3)])\n",
    "y = sc.parallelize([('a',3),('a',4),('b',5), ('c',1)])\n",
    "z = x.rightOuterJoin(y)\n",
    "print (z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partitionBy\n",
    "# 可以将RDD进行分区，然后重新生成一个ShuffleRDD,进行一个shuffle操作，对后面进行频繁的shuffle操作可以加快效率.\n",
    "\n",
    "x = sc.parallelize([(0,1),(1,2),(2,3)],2)\n",
    "y = x.partitionBy(numPartitions = 3, partitionFunc = lambda x: x)  # only key is passed to paritionFunc\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PythonRDD[57] at RDD at PythonRDD.scala:48, PythonRDD[58] at RDD at PythonRDD.scala:48, PythonRDD[59] at RDD at PythonRDD.scala:48, PythonRDD[60] at RDD at PythonRDD.scala:48, PythonRDD[61] at RDD at PythonRDD.scala:48]\n",
      "x = [0, 1, 2, 3, 4, 5, 6]\n",
      "sample:0 y = [0, 1, 2, 6]\n",
      "sample:1 y = [5]\n",
      "sample:2 y = [0, 1, 4, 5]\n",
      "sample:3 y = [0, 1, 2]\n",
      "sample:4 y = [0, 1, 2, 4, 5]\n",
      "[0, 5, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.sample(withReplacement=False, fraction=0.5) for i in range(5)] # call 'sample' 5 times\n",
    "# True表示有放回取样，false表示无放回\n",
    "print (ylist)\n",
    "print('x = ' + str(x.collect()))\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print ('sample:' + str(cnt) + ' y = ' +  str(y.collect()))\n",
    "\n",
    "sample = x.sample(withReplacement=True, fraction=0.5, seed=4).collect()\n",
    "print (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0, 1, 2, 3, 4, 5, 6]\n",
      "sample:0 y = [0, 6, 2]\n",
      "sample:1 y = [0, 4, 6]\n",
      "sample:2 y = [0, 1, 6]\n",
      "sample:3 y = [2, 5, 3]\n",
      "sample:4 y = [5, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# takeSample 和sample同发相似，但是第二个参数换了变量，返回的是Collection并非RDD.\n",
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  # call 'sample' 5 times\n",
    "print('x = ' + str(x.collect()))\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B', 'D', 'C', 'A']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['D','C','A'])\n",
    "# union 并集 不去重\n",
    "z = x.union(y)\n",
    "print(z.collect())\n",
    "\n",
    "# intersection 交集 去重\n",
    "z = x.intersection(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D')]\n"
     ]
    }
   ],
   "source": [
    "# cartesian 笛卡尔积 不去重\n",
    "x = sc.parallelize(['A','B'])\n",
    "y = sc.parallelize(['C','D'])\n",
    "z = x.cartesian(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join\n",
    "# 相当于inner join. 对两个需要连接的RDD进行cogroup，然后对每个Key下面的list进行笛卡尔积的操作，输出\n",
    "# 两两相交两个集合作为value ，相当于sql中where a.key= b.key。\n",
    "\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n",
      "[('B', (3, 7)), ('C', (4, None)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n",
      "[('B', (3, 7)), ('D', (None, 5)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n",
      "[('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)]) \n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)]) \n",
    "z = x.join(y) \n",
    "z1 = x.leftOuterJoin(y)\n",
    "z2 = x.rightOuterJoin(y)\n",
    "\n",
    "print(z.collect())\n",
    "print(z1.collect())\n",
    "print(z2.collect())\n",
    "print(z.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "\n",
    "sorted([(x, sorted(y)) for (x, y) in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[221] at parallelize at PythonRDD.scala:489\n",
      "[1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Reduce比groupby好，能用reduce就用reduce\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.reduce(lambda obj, accumulated: obj + accumulated)  # computes a cumulative sum\n",
    "print (x)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "# Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral “zero value.”\n",
    "x = sc.parallelize([1,2,3])\n",
    "neutral_zero_value = 0  # 0 for sum, 1 for multiplication\n",
    "y = x.fold(neutral_zero_value,lambda obj, accumulated: accumulated + obj) # computes cumulative sum\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "(9, 24)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate  聚合每个partition的元素，然后再聚合每个partition\n",
    "x = sc.parallelize([2,3,4])\n",
    "neutral_zero_value = (0,1) # sum: x+0 = x, product: 1*x = x\n",
    "seqOp = (lambda aggregated, el: (aggregated[0] + el, aggregated[1] * el)) \n",
    "combOp = (lambda aggregated, el: (aggregated[0] + el[0], aggregated[1] * el[1]))\n",
    "y = x.aggregate(neutral_zero_value,seqOp,combOp)  # computes (cumulative sum, cumulative product)\n",
    "print(x.collect())\n",
    "print(y)collectAsMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.907119849998599"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min，max，sum，count 直接根据单词理解即可\n",
    "data = sc.textFile(\"file:///home/lygbug666/workdir/spark-py-notebooks/average.txt\")\n",
    "data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x))).count()\n",
    "data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x))).sum()\n",
    "\n",
    "#mean 求平均值 variance方差 stdev偏差\n",
    "data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x))).mean()\n",
    "data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x))).variance()\n",
    "data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x))).stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "defaultdict(<class 'int'>, {1: 2, 3: 2, 2: 1})\n",
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "defaultdict(<class 'int'>, {'B': 2, 'A': 3})\n"
     ]
    }
   ],
   "source": [
    "# countByValue 统计元素在element当中出现的频率\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.countByValue()\n",
    "print(x.collect())\n",
    "print(y)\n",
    "\n",
    "# countByKey 通过Key进行count\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.countByKey()\n",
    "print(x.collect())\n",
    "print(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 3), ('A', 1), ('B', 2)]\n",
      "{'C': 3, 'A': 1, 'B': 2}\n"
     ]
    }
   ],
   "source": [
    "#collectAsMap 返回Key-v结构元素的value\n",
    "\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.collectAsMap()\n",
    "print(x.collect())\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[(1, 1), (4, 2), (9, 3)]\n"
     ]
    }
   ],
   "source": [
    "# KeyBy\n",
    "\n",
    "# 为RDD当中的elements创建一个元组\n",
    "\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.keyBy(lambda x: x**2)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keys 返回一个RDD的keys\n",
    "m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
    "m.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 3), ('A', 1), ('B', 2)]\n",
      "[3, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Values 返回一个RDD的values\n",
    "\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.values()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('A', 2), ('C', 3)]\n",
      "[('A', 2), ('B', 1), ('C', 3)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey\n",
    "x = sc.parallelize([('B',1),('A',2),('C',3)])\n",
    "y = x.sortByKey()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', 3), ('A', 12)]\n"
     ]
    }
   ],
   "source": [
    "# ReduceBykey\n",
    "#针对KV结构的数据将相同的key的values放到一起，与groupByKey不同，会进行一个类似mapreduce中的combine操作，减少相应数据的IO操作，加快效率. \n",
    "# 如果想进行一些非叠加操作，我们可以将value组合成字符串或其他格式将相同key的values组合在一起，通过迭代组合数据拆开操作.\n",
    "\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)], [(1, 2), (2, 3)]]\n",
      "[[(0, 1)], [(1, 2)], [(2, 3)]]\n"
     ]
    }
   ],
   "source": [
    "# partitionBy\n",
    "# 可以将RDD进行分区，然后重新生成一个ShuffleRDD,进行一个shuffle操作，对后面进行频繁的shuffle操作可以加快效率.\n",
    "\n",
    "x = sc.parallelize([(0,1),(1,2),(2,3)],2)\n",
    "y = x.partitionBy(numPartitions = 3, partitionFunc = lambda x: x)  # only key is passed to paritionFunc\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', [(1, 1), (2, 4)]), ('A', [(3, 9), (4, 16), (5, 25)])]\n"
     ]
    }
   ],
   "source": [
    "# combineByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "createCombiner = (lambda el: [(el,el**2)]) \n",
    "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)]) # append to aggregated\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )  # append agg1 with agg2\n",
    "y = x.combineByKey(createCombiner,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遍历RDD[(K,V)]中每一个元素\n",
    "\n",
    "1、如果当前K是一个新元素，使用createCombiner()创建K为键的累加器初始值，生成列表[('A',(3,9))], [('B',(1,1))]\n",
    "\n",
    "2、如果当前K已经遇到过，使用mergeValue()将当前(K,V)合并进第1步生成的累加器列表,，生成[('A',(3,9)),('A',(4,16)),('A',(5,25)),('B',(1,1,)), ('B', (2,4))],否则执行第1步\n",
    "\n",
    "3、将相同键值的累加器进行合并，得到[('A', [(3, 9), (4, 16), (5, 25)]), ('B', [(1, 1), (2, 4)])]\n",
    "\n",
    "因此得到如下结果：\n",
    "\n",
    "[('A', [(3, 9), (4, 16), (5, 25)]), ('B', [(1, 1), (2, 4)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', [(1, 1), (2, 4)]), ('A', [(3, 9), (4, 16), (5, 25)])]\n"
     ]
    }
   ],
   "source": [
    "# aggregateByKey\n",
    "# aggregateByKey 把类型为KV的RDD类型转为K U的RDD，V和U的类型可以不一样\n",
    "\n",
    "# aggregateByKey 内部是通过调用 combineByKey 来实现的，combineByKey 的 createCombine 函数逻辑由 zeroValue 这个变量实现，\n",
    "# zeroValue 作为聚合的初始值，乘法聚合则为1，集合操作则为空集合\n",
    "# seqOp在combineByKey中的功能是mergeValues，(U,V)=>U\n",
    "# combOp在combineByKey中的功能是mergeCombiners\n",
    "\n",
    "# 从求均值的实现来看，aggregate通过提供零值的方式，避免了combineByKey中的createCombiner步骤\n",
    "# (createCombiner本质工作就是遇到第一个key时进行初始化操作，这个初始化不是提供零值，而是对第一个(k,v)进行转换得到c的初始值）\n",
    "\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = [] # empty list is 'zero value' for append operation\n",
    "seqOp = (lambda aggregated, el: aggregated + [(el,el**2)])\n",
    "combOp = (lambda agg1,agg2: agg1 + agg2 )\n",
    "y = x.aggregateByKey(zeroValue,seqOp,combOp)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', 2), ('A', 60)]\n"
     ]
    }
   ],
   "source": [
    "# foldByKey\n",
    "# foldByKey 合并每一个 key 的所有值，在级联函数和“零值”中使用。\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = 1 # one is 'zero value' for multiplication\n",
    "y = x.foldByKey(zeroValue,lambda agg,x: agg*x )  # computes cumulative product within each key\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
      "[('A', [1, 4, 9]), ('B', [16, 25])]\n"
     ]
    }
   ],
   "source": [
    "# mapValues\n",
    "#　针对KV数据，对数据中的value进行map操作，而不对key进行处理\n",
    "# 每一个元素的value被输入函数映射为一系列的值，然后这些值再与原RDD中的key组成一系列新的KV对.\n",
    "\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.mapValues(lambda x: [i**2 for i in x]) # function is applied to entire value\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
      "[('A', 1), ('A', 4), ('A', 9), ('B', 16), ('B', 25)]\n"
     ]
    }
   ],
   "source": [
    "# flatMapValues\n",
    "\n",
    "# 类似于把一个[('A', (1, 2, 3)), ('B', (4, 5))] flatmap成以下这种形式\n",
    "# [('A', 1), ('A', 4), ('A', 9), ('B', 16), ('B', 25)]\n",
    "\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))]) \n",
    "y = x.flatMapValues(lambda x: [i**2 for i in x]) \n",
    "# function is applied to entire value, then result is flattened \n",
    "print(x.collect()) \n",
    "print(y.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', (3, 3)), ('A', 2), ('A', (1, 1))]\n",
      "[('B', (7, 7)), ('A', 6), ('D', (5, 5))]\n",
      "[('D', 9), ('B', (8, 8))]\n",
      "B [[(3, 3)], [(7, 7)], [(8, 8)]]\n",
      "A [[2, (1, 1)], [6], []]\n",
      "D [[], [(5, 5)], [9]]\n",
      "C [[4], [], []]\n"
     ]
    }
   ],
   "source": [
    "# groupWith\n",
    "# 在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。\n",
    "# 这个操作在其它框架上称为CoGroup\n",
    "\n",
    "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
    "y = sc.parallelize([('B',(7,7)),('A',6),('D',(5,5))])\n",
    "z = sc.parallelize([('D',9),('B',(8,8))])\n",
    "a = x.groupWith(y,z)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())\n",
    "for key,val in list(a.collect()):\n",
    "    print(key, [list(i) for i in val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', (3, 3)), ('A', 2), ('A', (1, 1))]\n",
      "[('A', 8), ('B', 7), ('A', 6), ('D', (5, 5))]\n",
      "B [[(3, 3)], [7]]\n",
      "D [[], [(5, 5)]]\n",
      "C [[4], []]\n",
      "A [[2, (1, 1)], [8, 6]]\n"
     ]
    }
   ],
   "source": [
    "# cogroup\n",
    "# 对两个RDD中的KV元素，每个RDD中相同key中的元素分别聚合成一个集合，与reduceByKey不同的是cogroup针对两个RDD中相同的key元素进行合并.\n",
    "# 不同RDD当中的KV元素分别 放在不同的集合当中.\n",
    "\n",
    "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',(5,5))])\n",
    "z = x.cogroup(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "for key,val in list(z.collect()):\n",
    "    print(key, [list(i) for i in val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 1), ('B', 2), ('C', 3), ('B', 4), ('A', 5)]\n",
      "[('A', 1), ('B', 2), ('B', 4)]\n"
     ]
    }
   ],
   "source": [
    "# sampleByKey\n",
    "#返回RDD的一个子集合，通过Key采样.\n",
    "\n",
    "x = sc.parallelize([('A',1),('B',2),('C',3),('B',4),('A',5)])\n",
    "y = x.sampleByKey(withReplacement=False, fractions={'A':0.5, 'B':1, 'C':0.2})\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('b', 5), ('e', 10)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtractByKey\n",
    "# 返回KV对类型，x-y 剩下的key-value\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2),(\"c\",5),(\"d\",6),(\"e\",10)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None),(\"d\",8)])\n",
    "sorted(x.subtractByKey(y).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "[('C', 8), ('A', 2), ('D', 1)]\n",
      "[('C', 4), ('A', 1), ('B', 3)]\n"
     ]
    }
   ],
   "source": [
    "# subtract\n",
    "# 对RDD1中取出RDD1于RDD2交集中的所有元素.\n",
    "\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('C',8),('A',2),('D',1)])\n",
    "z = x.subtract(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n",
      "[[1, 2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "# repartition 可以在最初对RDD进行分区操作，这个操作实际上是一个shuffle，可能比较耗时，但是如果之后的action比较多的话，\n",
    "# 可以减少以后的操作时间，其中N值看这个cpu的个数，一般大于2倍cpu，小于1000\n",
    "\n",
    "# coalesce \n",
    "# 返回一个新的RDD，经过reduce操作后到一个numPartitions 的分区当中,可能需要shuffle\n",
    "# you go from 1000 partitions to 100 partitions, there will not be a shuffle, \n",
    "# instead each of the 100 new partitions will claim 10 of the current partitions.\n",
    "\n",
    "print (sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect())\n",
    "print (sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect())\n",
    "\n",
    "# glom() 返回一个RDD 用于分组一个RDD到一个list\n",
    "# glom() flattens elements on the same partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'A', 'A']\n",
      "[66, 65, 65]\n",
      "[('B', 66), ('A', 65), ('A', 65)]\n",
      "[('B', 0), ('A', 1), ('A', 2)]\n",
      "[('B', 2), ('A', 5), ('A', 7)]\n"
     ]
    }
   ],
   "source": [
    "# ZIP\n",
    "# zipWithUniqueId\n",
    "# zipWithIndex\n",
    "# zip expects x and y to have same #partitions and #elements/partition\n",
    "\n",
    "x = sc.parallelize(['B','A','A'])\n",
    "y = x.map(lambda x: ord(x))  \n",
    "z = x.zip(y)\n",
    "z1 = x.zipWithIndex()\n",
    "z2 = x.zipWithUniqueId()\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())\n",
    "print(z1.collect())\n",
    "print(z2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"file:///home/lygbug666/workdir/spark-py-notebooks/word.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount = data.flatMap(lambda line : line.split(\" \")).map(lambda x: (x,1)).reduceByKey(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lyc', 1), ('abc', 2), ('xxx', 2), ('bug', 1), ('add', 1), ('fsdf', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"file:///home/lygbug666/workdir/spark-py-notebooks/average.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 1), (20, 1), (20, 1), (25, 1), (30, 1), (60, 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x),1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 6)\n"
     ]
    }
   ],
   "source": [
    "average_tuple = data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x),1)).reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "print (average_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.333333333333332\n"
     ]
    }
   ],
   "source": [
    "average = average_tuple[0]/average_tuple[1]\n",
    "print (average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "print (data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x))).reduce(lambda x,y : x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average = 170/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"spark\",2),(\"hadoop\",6),(\"hadoop\",4),(\"spark\",6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', (2, 1)), ('hadoop', (6, 1)), ('hadoop', (4, 1)), ('spark', (6, 1))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x: (x,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', (10, 2)), ('spark', (8, 2))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x: (x,1)).reduceByKey(lambda x,y : (x[0]+y[0], x[1]+y[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', 5.0), ('spark', 4.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x: (x,1)).reduceByKey(lambda x,y : (x[0]+y[0], x[1]+y[1])).mapValues(lambda x: (x[0]/x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'hadoop': 2, 'spark': 2})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('hadoop', 4): 1,\n",
       "             ('hadoop', 6): 1,\n",
       "             ('spark', 2): 1,\n",
       "             ('spark', 6): 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"file:///home/lygbug666/workdir/spark-py-notebooks/average.txt\").flatMap(lambda line: line.split(\" \")).map(lambda x: (int(x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 1), (20, 1), (20, 1), (25, 1), (30, 1), (60, 1)]\n"
     ]
    }
   ],
   "source": [
    "print (data.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 6)\n"
     ]
    }
   ],
   "source": [
    "average_tuple1=data.aggregate((0,0),(lambda acc,value : (acc[0]+value[0],acc[1]+1)),(lambda acc1,acc2: (acc1[0]+acc2[0],acc1[1]+acc2[1])))\n",
    "print (average_tuple1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.333333333333332\n"
     ]
    }
   ],
   "source": [
    "print (average_tuple1[0]/average_tuple1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark创建DataFrame的三种方法\n",
    "\n",
    "1.Spark中使用toDF函数创建DataFrame\n",
    "\n",
    "2.Spark中使用createDataFrame函数创建DataFrame\n",
    "\n",
    "3.通过文件直接创建DataFrame\n",
    "\n",
    "val df = sqlContext.read.parquet(\"hdfs:/path/to/file\")\n",
    "\n",
    "val df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "Pyspark\n",
    "\n",
    "1.使用反射推断schema：\n",
    "\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "\n",
    "schemaPeople.registerTempTable(\"people\")\n",
    "\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "2.使用编程方式指定\n",
    "\n",
    "fields = [StructField(field_name1, StringType(), True) , StructField(field_name2, StringType(), True)]\n",
    "\n",
    "schema = StructType(fields)\n",
    "\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "dataframe实际上就是一张表，可以使用\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"file:///home/lygbug666/workdir/spark-py-notebooks/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5])\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df = sqlContext.createDataFrame(row_data)\n",
    "\n",
    "interactions_df.registerTempTable(\"interactions\")\n",
    "\n",
    "interactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.createDataFrame([(0, \"a b c d e spark\", 1.0), \n",
    "                                  (1, \"b d\", 0.0), (2, \"spark f g h\", 1.0), \n",
    "                                  (3, \"hadoop mapreduce\", 0.0)], \n",
    "                                 [\"id\", \"text\", \"label\"])\n",
    "\n",
    "training.registerTempTable(\"train\")\n",
    "\n",
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "lines = sc.textFile(\"file:///home/lygbug666/workdir/spark-py-notebooks/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "schemaPeople.show()\n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///home/lygbug666/workdir/spark-py-notebooks/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "schema1 = StructType([StructField(\"name\", StringType(), True), StructField(\"age\", StringType(), True)])\n",
    "\n",
    "schemaPeople1 = spark.createDataFrame(people, schema1)\n",
    "\n",
    "schemaPeople1.createOrReplaceTempView(\"people1\")\n",
    "\n",
    "results1 = spark.sql(\"SELECT name FROM people1\")\n",
    "results1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key1: string (nullable = true)\n",
      " |-- key2: long (nullable = true)\n",
      " |-- key3: long (nullable = true)\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| aaa|   1|   2|\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training1 = spark.createDataFrame([(\"aaa\",1,2),(\"bbb\",3,4),(\"ccc\",3,5),(\"bbb\",4, 6)], \n",
    "                                 [\"key1\", \"key2\", \"key3\"])\n",
    "\n",
    "training1.registerTempTable(\"train1\")\n",
    "\n",
    "training1.printSchema()\n",
    "\n",
    "training1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 选择 select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| aaa|   1|   2|\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| aaa|   1|   2|\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| aaa|   1|   2|\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training1.select(\"key1\",\"key2\", \"key3\").show()\n",
    "training1.select(\"*\").show()\n",
    "\n",
    "sqlContext.sql(\"select * from train1\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 过滤 where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter 函数\n",
    "\n",
    "training1.filter(training1.key1>\"aaa\").show()\n",
    "training1.filter(\"key1>'aaa'\").show()\n",
    "\n",
    "\n",
    "sqlContext.sql(\"select * from train1 where key1 > 'aaa'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where函数\n",
    "\n",
    "training1.where(training1.key1>\"aaa\").show()\n",
    "training1.where(\"key1>'aaa'\").show()\n",
    "\n",
    "sqlContext.sql(\"select * from train1 where key1 > 'aaa'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 分组，聚合，排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|key1|count|\n",
      "+----+-----+\n",
      "| aaa|    1|\n",
      "| bbb|    2|\n",
      "| ccc|    1|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|key1|count|\n",
      "+----+-----+\n",
      "| ccc|    1|\n",
      "| bbb|    2|\n",
      "| aaa|    1|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|key1|count|\n",
      "+----+-----+\n",
      "| ccc|    1|\n",
      "| aaa|    1|\n",
      "| bbb|    2|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|key1|count|\n",
      "+----+-----+\n",
      "| ccc|    1|\n",
      "| bbb|    2|\n",
      "| aaa|    1|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|key1|count|\n",
      "+----+-----+\n",
      "| bbb|    2|\n",
      "| ccc|    1|\n",
      "| aaa|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组计数\n",
    "\n",
    "training1.groupBy(\"key1\").count().sort(\"key1\").show()\n",
    "training1.groupBy(training1.key1).count().sort(training1.key1.desc()).show()\n",
    "training1.groupBy(\"key1\").count().sort(\"count\").show()\n",
    "\n",
    "#上面代码中的count不是记录数，而是对groupBy的聚合结果的计数。\n",
    "\n",
    "sqlContext.sql(\"select key1, count(*) as count from train1 group by key1 order by key1 desc\").show()\n",
    "sqlContext.sql(\"select key1, count(*) as count from train1 group by key1 order by count(*) desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|key1|\n",
      "+----+\n",
      "| ccc|\n",
      "| aaa|\n",
      "| bbb|\n",
      "+----+\n",
      "\n",
      "+----+\n",
      "|key1|\n",
      "+----+\n",
      "| ccc|\n",
      "| aaa|\n",
      "| bbb|\n",
      "+----+\n",
      "\n",
      "+--------------------+\n",
      "|count(DISTINCT key1)|\n",
      "+--------------------+\n",
      "|                   3|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 如果是要看分组后有多少条记录，代码如下\n",
    "training1.select(\"key1\").distinct().show()\n",
    "sqlContext.sql(\"select distinct key1 from train1\").show()\n",
    "\n",
    "\n",
    "training1.select(\"key1\").distinct().count()\n",
    "sqlContext.sql(\"select count(distinct key1) from train1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|key1|avg(key2)|\n",
      "+----+---------+\n",
      "| ccc|      3.0|\n",
      "| aaa|      1.0|\n",
      "| bbb|      3.5|\n",
      "+----+---------+\n",
      "\n",
      "+----+---------+\n",
      "|key1|max(key2)|\n",
      "+----+---------+\n",
      "| ccc|        3|\n",
      "| aaa|        1|\n",
      "| bbb|        4|\n",
      "+----+---------+\n",
      "\n",
      "+---------+---------+\n",
      "|avg(key2)|avg(key3)|\n",
      "+---------+---------+\n",
      "|     2.75|     4.25|\n",
      "+---------+---------+\n",
      "\n",
      "+----+----+-----+\n",
      "|key1|key2|count|\n",
      "+----+----+-----+\n",
      "| aaa|   1|    1|\n",
      "| ccc|   3|    1|\n",
      "| bbb|   4|    1|\n",
      "| bbb|   3|    1|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training1.groupBy(\"key1\").agg({'key2': 'mean'}).collect()\n",
    "training1.groupBy(\"key1\").agg({'key2': 'mean'}).show()\n",
    "\n",
    "training1.groupBy(\"key1\").agg({'key2': 'max'}).show()\n",
    "training1.groupBy().avg().show()\n",
    "\n",
    "training1.groupBy([\"key1\", training1.key2]).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------+---------+\n",
      "|key1|count(key1)|max(key2)|avg(key3)|\n",
      "+----+-----------+---------+---------+\n",
      "| ccc|          1|        3|      5.0|\n",
      "| aaa|          1|        1|      2.0|\n",
      "| bbb|          2|        4|      5.0|\n",
      "+----+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select key1, count(key1), max(key2), avg(key3) from train1 group by key1\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5)多表操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key1: string (nullable = true)\n",
      " |-- key2: long (nullable = true)\n",
      " |-- key3: long (nullable = true)\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| aaa|   1|   2|\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n",
      "root\n",
      " |-- key1: string (nullable = true)\n",
      " |-- key2: long (nullable = true)\n",
      " |-- key4: long (nullable = true)\n",
      "\n",
      "+----+----+----+\n",
      "|key1|key2|key4|\n",
      "+----+----+----+\n",
      "| aaa|   2|   2|\n",
      "| bbb|   3|   5|\n",
      "| ddd|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "| eee|   1|   2|\n",
      "| aaa|   1|   5|\n",
      "| fff|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(\"aaa\", 1, 2), (\"bbb\", 3, 4), (\"ccc\", 3, 5), (\"bbb\", 4, 6)], \n",
    "                                 [\"key1\", \"key2\", \"key3\"])\n",
    "\n",
    "df1.registerTempTable(\"df1\")\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame([(\"aaa\", 2, 2),(\"bbb\", 3, 5),(\"ddd\", 3, 5),(\"bbb\", 4, 6),(\"eee\", 1, 2),(\"aaa\", 1, 5),(\"fff\",5,6)], \n",
    "                                 [\"key1\", \"key2\", \"key4\"])\n",
    "\n",
    "df2.registerTempTable(\"df2\")\n",
    "\n",
    "df2.printSchema()\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|key1|key2|key3|key2|key4|\n",
      "+----+----+----+----+----+\n",
      "| aaa|   1|   2|   2|   2|\n",
      "| aaa|   1|   2|   1|   5|\n",
      "| bbb|   3|   4|   3|   5|\n",
      "| bbb|   3|   4|   4|   6|\n",
      "| bbb|   4|   6|   3|   5|\n",
      "| bbb|   4|   6|   4|   6|\n",
      "+----+----+----+----+----+\n",
      "\n",
      "+----+----+----+----+----+----+\n",
      "|key1|key2|key3|key1|key2|key4|\n",
      "+----+----+----+----+----+----+\n",
      "| aaa|   1|   2| aaa|   2|   2|\n",
      "| aaa|   1|   2| aaa|   1|   5|\n",
      "| bbb|   3|   4| bbb|   3|   5|\n",
      "| bbb|   3|   4| bbb|   4|   6|\n",
      "| bbb|   4|   6| bbb|   3|   5|\n",
      "| bbb|   4|   6| bbb|   4|   6|\n",
      "+----+----+----+----+----+----+\n",
      "\n",
      "+----+----+----+----+----+----+\n",
      "|key1|key2|key3|key1|key2|key4|\n",
      "+----+----+----+----+----+----+\n",
      "| aaa|   1|   2| aaa|   2|   2|\n",
      "| aaa|   1|   2| aaa|   1|   5|\n",
      "| bbb|   3|   4| bbb|   3|   5|\n",
      "| bbb|   3|   4| bbb|   4|   6|\n",
      "| bbb|   4|   6| bbb|   3|   5|\n",
      "| bbb|   4|   6| bbb|   4|   6|\n",
      "+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.join(df2, \"key1\")\n",
    "df3.show()\n",
    "\n",
    "sqlContext.sql(\"select * from df1, df2 where df1.key1 = df2.key1\").show()\n",
    "\n",
    "sqlContext.sql(\"select * from df1 join df2 on df1.key1 = df2.key1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'resolved attribute(s) key1#1629 missing from key2#1643L,key4#1644L,key3#1631L,key2#1630L,key1#1835 in operator !Project [key1#1629, key2#1630L, key3#1631L];;\\n!Project [key1#1629, key2#1630L, key3#1631L]\\n+- Project [coalesce(key1#1629, key1#1642) AS key1#1835, key2#1630L, key3#1631L, key2#1643L, key4#1644L]\\n   +- Join FullOuter, (key1#1629 = key1#1642)\\n      :- LogicalRDD [key1#1629, key2#1630L, key3#1631L]\\n      +- LogicalRDD [key1#1642, key2#1643L, key4#1644L]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/software/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4502.select.\n: org.apache.spark.sql.AnalysisException: resolved attribute(s) key1#1629 missing from key2#1643L,key4#1644L,key3#1631L,key2#1630L,key1#1835 in operator !Project [key1#1629, key2#1630L, key3#1631L];;\n!Project [key1#1629, key2#1630L, key3#1631L]\n+- Project [coalesce(key1#1629, key1#1642) AS key1#1835, key2#1630L, key3#1631L, key2#1643L, key4#1644L]\n   +- Join FullOuter, (key1#1629 = key1#1642)\n      :- LogicalRDD [key1#1629, key2#1630L, key3#1631L]\n      +- LogicalRDD [key1#1642, key2#1643L, key4#1644L]\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2884)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1150)\n\tat sun.reflect.GeneratedMethodAccessor293.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-3d59be5e9378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"key1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"key1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \"\"\"\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'resolved attribute(s) key1#1629 missing from key2#1643L,key4#1644L,key3#1631L,key2#1630L,key1#1835 in operator !Project [key1#1629, key2#1630L, key3#1631L];;\\n!Project [key1#1629, key2#1630L, key3#1631L]\\n+- Project [coalesce(key1#1629, key1#1642) AS key1#1835, key2#1630L, key3#1631L, key2#1643L, key4#1644L]\\n   +- Join FullOuter, (key1#1629 = key1#1642)\\n      :- LogicalRDD [key1#1629, key2#1630L, key3#1631L]\\n      +- LogicalRDD [key1#1642, key2#1643L, key4#1644L]\\n'"
     ]
    }
   ],
   "source": [
    "df4 = df1.join(df2, \"key1\", \"outer\").select(df1.key1,df1.key2,df1.key3).collect()\n",
    "df4 = df1.join(df2, \"key1\", \"outer\")\n",
    "\n",
    "df4.show()\n",
    "\n",
    "sqlContext.sql(\"select * from df1 outer join df2 on df1.key1 = df2.key1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|key1|key2|key3|key2|key4|\n",
      "+----+----+----+----+----+\n",
      "| ccc|   3|   5|null|null|\n",
      "| aaa|   1|   2|   2|   2|\n",
      "| aaa|   1|   2|   1|   5|\n",
      "| bbb|   3|   4|   3|   5|\n",
      "| bbb|   3|   4|   4|   6|\n",
      "| bbb|   4|   6|   3|   5|\n",
      "| bbb|   4|   6|   4|   6|\n",
      "+----+----+----+----+----+\n",
      "\n",
      "+----+----+----+----+----+----+\n",
      "|key1|key2|key3|key1|key2|key4|\n",
      "+----+----+----+----+----+----+\n",
      "| aaa|   1|   2| aaa|   2|   2|\n",
      "| aaa|   1|   2| aaa|   1|   5|\n",
      "| bbb|   3|   4| bbb|   3|   5|\n",
      "| bbb|   4|   6| bbb|   3|   5|\n",
      "| bbb|   3|   4| bbb|   4|   6|\n",
      "| bbb|   4|   6| bbb|   4|   6|\n",
      "| ccc|   3|   5|null|null|null|\n",
      "+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df1.join(df2, \"key1\", \"left_outer\")\n",
    "df5.show()\n",
    "\n",
    "sqlContext.sql(\"select * from df1 left outer join df2 on df1.key1 = df2.key1 order by df1.key1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|key1|key2|key3|key2|key4|\n",
      "+----+----+----+----+----+\n",
      "| aaa|   1|   2|   2|   2|\n",
      "| aaa|   1|   2|   1|   5|\n",
      "| bbb|   3|   4|   3|   5|\n",
      "| bbb|   3|   4|   4|   6|\n",
      "| bbb|   4|   6|   4|   6|\n",
      "| bbb|   4|   6|   3|   5|\n",
      "| ddd|null|null|   3|   5|\n",
      "| eee|null|null|   1|   2|\n",
      "| fff|null|null|   5|   6|\n",
      "+----+----+----+----+----+\n",
      "\n",
      "+----+----+----+----+----+----+\n",
      "|key1|key2|key3|key1|key2|key4|\n",
      "+----+----+----+----+----+----+\n",
      "| aaa|   1|   2| aaa|   2|   2|\n",
      "| aaa|   1|   2| aaa|   1|   5|\n",
      "| bbb|   3|   4| bbb|   4|   6|\n",
      "| bbb|   4|   6| bbb|   3|   5|\n",
      "| bbb|   4|   6| bbb|   4|   6|\n",
      "| bbb|   3|   4| bbb|   3|   5|\n",
      "|null|null|null| ddd|   3|   5|\n",
      "|null|null|null| eee|   1|   2|\n",
      "|null|null|null| fff|   5|   6|\n",
      "+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = df1.join(df2, \"key1\", \"right_outer\").sort(df2.key1)\n",
    "df6.show()\n",
    "\n",
    "sqlContext.sql(\"select * from df1 right outer join df2 on df1.key1 = df2.key1 order by df2.key1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| aaa|   1|   2|\n",
      "| bbb|   3|   4|\n",
      "| bbb|   4|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7 = df1.join(df2, \"key1\", \"left_Semi\")\n",
    "df7.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+\n",
      "|key1|key2|key3|key1|key2|key4|\n",
      "+----+----+----+----+----+----+\n",
      "| aaa|   1|   2| aaa|   2|   2|\n",
      "| aaa|   1|   2| bbb|   3|   5|\n",
      "| aaa|   1|   2| ddd|   3|   5|\n",
      "| aaa|   1|   2| bbb|   4|   6|\n",
      "| aaa|   1|   2| eee|   1|   2|\n",
      "| aaa|   1|   2| aaa|   1|   5|\n",
      "| aaa|   1|   2| fff|   5|   6|\n",
      "| bbb|   3|   4| aaa|   2|   2|\n",
      "| bbb|   3|   4| bbb|   3|   5|\n",
      "| bbb|   3|   4| ddd|   3|   5|\n",
      "+----+----+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8 = df1.crossJoin(df2)\n",
    "df8.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|key1|key2|key3|key4|\n",
      "+----+----+----+----+\n",
      "| aaa|   1|   2|   5|\n",
      "| bbb|   4|   6|   6|\n",
      "| bbb|   3|   4|   5|\n",
      "+----+----+----+----+\n",
      "\n",
      "+----+----+----+----+----+----+\n",
      "|key1|key2|key3|key1|key2|key4|\n",
      "+----+----+----+----+----+----+\n",
      "| aaa|   1|   2| aaa|   1|   5|\n",
      "| bbb|   4|   6| bbb|   4|   6|\n",
      "| bbb|   3|   4| bbb|   3|   5|\n",
      "+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9 = df1.join(df2, [\"key1\", \"key2\"])\n",
    "df9.show()\n",
    "\n",
    "sqlContext.sql(\"select * from df1 join df2 on df1.key1=df2.key1 and df1.key2=df2.key2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|key1|key2|key3|\n",
      "+----+----+----+\n",
      "| aaa|   1|   2|\n",
      "| bbb|   3|   4|\n",
      "| ccc|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "| aaa|   2|   2|\n",
      "| bbb|   3|   5|\n",
      "| ddd|   3|   5|\n",
      "| bbb|   4|   6|\n",
      "| eee|   1|   2|\n",
      "| aaa|   1|   5|\n",
      "| fff|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10 = df1.union(df2)\n",
    "\n",
    "df10.show()\n",
    "\n",
    "df10.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Inverted_index in many file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id5',\n",
       "  'hadoop is good in world\\nspark is very famous in world\\nI am rabbit\\nyou are cat\\n'),\n",
       " ('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id3',\n",
       "  'hello cat\\nhello rabbit\\ncat is doing spark\\ncat is the world\\nspark is good\\n'),\n",
       " ('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id2',\n",
       "  'hello spark\\nthe spark app for you\\nI love you\\nyou are a cat\\n'),\n",
       " ('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id4',\n",
       "  'spark is better than hadoop\\nI love hadoop\\nyou love spark\\ncat and rabbit love app\\n'),\n",
       " ('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id1',\n",
       "  'hello world\\nhello hadoop\\nhadoop love\\nlove cat\\ncat love rabbit\\n')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.wholeTextFiles(\"file:///home/lygbug666/workdir/spark-py-notebooks/inverted_index\")\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id5',\n",
       "  ['hadoop',\n",
       "   'is',\n",
       "   'good',\n",
       "   'in',\n",
       "   'world\\nspark',\n",
       "   'is',\n",
       "   'very',\n",
       "   'famous',\n",
       "   'in',\n",
       "   'world\\nI',\n",
       "   'am',\n",
       "   'rabbit\\nyou',\n",
       "   'are',\n",
       "   'cat\\n']),\n",
       " ('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id3',\n",
       "  ['hello',\n",
       "   'cat\\nhello',\n",
       "   'rabbit\\ncat',\n",
       "   'is',\n",
       "   'doing',\n",
       "   'spark\\ncat',\n",
       "   'is',\n",
       "   'the',\n",
       "   'world\\nspark',\n",
       "   'is',\n",
       "   'good\\n']),\n",
       " ('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id2',\n",
       "  ['hello',\n",
       "   'spark\\nthe',\n",
       "   'spark',\n",
       "   'app',\n",
       "   'for',\n",
       "   'you\\nI',\n",
       "   'love',\n",
       "   'you\\nyou',\n",
       "   'are',\n",
       "   'a',\n",
       "   'cat\\n']),\n",
       " ('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id4',\n",
       "  ['spark',\n",
       "   'is',\n",
       "   'better',\n",
       "   'than',\n",
       "   'hadoop\\nI',\n",
       "   'love',\n",
       "   'hadoop\\nyou',\n",
       "   'love',\n",
       "   'spark\\ncat',\n",
       "   'and',\n",
       "   'rabbit',\n",
       "   'love',\n",
       "   'app\\n']),\n",
       " ('file:/home/lygbug666/workdir/spark-py-notebooks/inverted_index/id1',\n",
       "  ['hello',\n",
       "   'world\\nhello',\n",
       "   'hadoop\\nhadoop',\n",
       "   'love\\nlove',\n",
       "   'cat\\ncat',\n",
       "   'love',\n",
       "   'rabbit\\n'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = data.mapValues(lambda line: line.split(\" \"))\n",
    "file_name.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Inverted_index in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1\\tApache Spark Scala Hadoop Java C Python Do And Will KNN',\n",
       " 'doc2\\tSVM Scala News Play Akka Yes GBDT',\n",
       " 'doc3\\tLDA SVM RF GBDT Adaboost Kmeans KNN',\n",
       " 'doc4\\tQQ BAT I Great All LDA',\n",
       " 'doc5\\tApache Hadoop MapReduce Git SVN SVM']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.textFile(\"file:///home/lygbug666/workdir/spark-py-notebooks/inverted_index/input\")\n",
    "data1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['doc1', 'Apache Spark Scala Hadoop Java C Python Do And Will KNN'],\n",
       " ['doc2', 'SVM Scala News Play Akka Yes GBDT'],\n",
       " ['doc3', 'LDA SVM RF GBDT Adaboost Kmeans KNN'],\n",
       " ['doc4', 'QQ BAT I Great All LDA'],\n",
       " ['doc5', 'Apache Hadoop MapReduce Git SVN SVM']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.map(lambda line: line.split(\"\\t\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doc1', 'Apache Spark Scala Hadoop Java C Python Do And Will KNN'),\n",
       " ('doc2', 'SVM Scala News Play Akka Yes GBDT'),\n",
       " ('doc3', 'LDA SVM RF GBDT Adaboost Kmeans KNN'),\n",
       " ('doc4', 'QQ BAT I Great All LDA'),\n",
       " ('doc5', 'Apache Hadoop MapReduce Git SVN SVM')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.map(lambda line: line.split(\"\\t\")).map(lambda x: (x[0],x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('doc1', ['Apache', 'Spark', 'Scala', 'Hadoop', 'Java', 'C', 'Python', 'Do', 'And', 'Will', 'KNN']), ('doc2', ['SVM', 'Scala', 'News', 'Play', 'Akka', 'Yes', 'GBDT']), ('doc3', ['LDA', 'SVM', 'RF', 'GBDT', 'Adaboost', 'Kmeans', 'KNN']), ('doc4', ['QQ', 'BAT', 'I', 'Great', 'All', 'LDA']), ('doc5', ['Apache', 'Hadoop', 'MapReduce', 'Git', 'SVN', 'SVM'])]\n"
     ]
    }
   ],
   "source": [
    "index = data1.map(lambda line: line.split(\"\\t\")).map(lambda x: (x[0],x[1].split(\" \")))\n",
    "print (index.collect())\n",
    "\n",
    "# 传统的正排索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Apache', 'Spark', 'Scala', 'Hadoop', 'Java', 'C', 'Python', 'Do', 'And', 'Will', 'KNN'], 'doc1'), (['SVM', 'Scala', 'News', 'Play', 'Akka', 'Yes', 'GBDT'], 'doc2'), (['LDA', 'SVM', 'RF', 'GBDT', 'Adaboost', 'Kmeans', 'KNN'], 'doc3'), (['QQ', 'BAT', 'I', 'Great', 'All', 'LDA'], 'doc4'), (['Apache', 'Hadoop', 'MapReduce', 'Git', 'SVN', 'SVM'], 'doc5')]\n"
     ]
    }
   ],
   "source": [
    "inverted_word = index.map(lambda x: (x[1],x[0]))\n",
    "\n",
    "print (inverted_index.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    list = []\n",
    "    for word in x[0]:\n",
    "        list.append((word,x[1]))\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Apache', 'doc1'), ('Spark', 'doc1'), ('Scala', 'doc1'), ('Hadoop', 'doc1'), ('Java', 'doc1'), ('C', 'doc1'), ('Python', 'doc1'), ('Do', 'doc1'), ('And', 'doc1'), ('Will', 'doc1'), ('KNN', 'doc1')], [('SVM', 'doc2'), ('Scala', 'doc2'), ('News', 'doc2'), ('Play', 'doc2'), ('Akka', 'doc2'), ('Yes', 'doc2'), ('GBDT', 'doc2')], [('LDA', 'doc3'), ('SVM', 'doc3'), ('RF', 'doc3'), ('GBDT', 'doc3'), ('Adaboost', 'doc3'), ('Kmeans', 'doc3'), ('KNN', 'doc3')], [('QQ', 'doc4'), ('BAT', 'doc4'), ('I', 'doc4'), ('Great', 'doc4'), ('All', 'doc4'), ('LDA', 'doc4')], [('Apache', 'doc5'), ('Hadoop', 'doc5'), ('MapReduce', 'doc5'), ('Git', 'doc5'), ('SVN', 'doc5'), ('SVM', 'doc5')]]\n"
     ]
    }
   ],
   "source": [
    "inverted_wordllist = inverted_word.map(lambda x: f(x))\n",
    "\n",
    "print (inverted_wordllist.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apache', 'doc1'), ('Spark', 'doc1'), ('Scala', 'doc1'), ('Hadoop', 'doc1'), ('Java', 'doc1'), ('C', 'doc1'), ('Python', 'doc1'), ('Do', 'doc1'), ('And', 'doc1'), ('Will', 'doc1'), ('KNN', 'doc1'), ('SVM', 'doc2'), ('Scala', 'doc2'), ('News', 'doc2'), ('Play', 'doc2'), ('Akka', 'doc2'), ('Yes', 'doc2'), ('GBDT', 'doc2'), ('LDA', 'doc3'), ('SVM', 'doc3'), ('RF', 'doc3'), ('GBDT', 'doc3'), ('Adaboost', 'doc3'), ('Kmeans', 'doc3'), ('KNN', 'doc3'), ('QQ', 'doc4'), ('BAT', 'doc4'), ('I', 'doc4'), ('Great', 'doc4'), ('All', 'doc4'), ('LDA', 'doc4'), ('Apache', 'doc5'), ('Hadoop', 'doc5'), ('MapReduce', 'doc5'), ('Git', 'doc5'), ('SVN', 'doc5'), ('SVM', 'doc5')]\n"
     ]
    }
   ],
   "source": [
    "inverted_index = inverted_wordllist.flatMap(lambda x: x)\n",
    "print (inverted_index.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apache', 'doc1|doc5'), ('Spark', 'doc1'), ('Java', 'doc1'), ('C', 'doc1'), ('Python', 'doc1'), ('Do', 'doc1'), ('And', 'doc1'), ('Will', 'doc1'), ('News', 'doc2'), ('QQ', 'doc4'), ('Great', 'doc4'), ('MapReduce', 'doc5'), ('SVN', 'doc5'), ('Scala', 'doc1|doc2'), ('Hadoop', 'doc1|doc5'), ('KNN', 'doc1|doc3'), ('SVM', 'doc2|doc3|doc5'), ('Play', 'doc2'), ('Akka', 'doc2'), ('Yes', 'doc2'), ('GBDT', 'doc2|doc3'), ('LDA', 'doc3|doc4'), ('RF', 'doc3'), ('Adaboost', 'doc3'), ('Kmeans', 'doc3'), ('BAT', 'doc4'), ('I', 'doc4'), ('All', 'doc4'), ('Git', 'doc5')]\n"
     ]
    }
   ],
   "source": [
    "inverted_index1 = inverted_index.reduceByKey(lambda x,y : x+ \"|\" +y)\n",
    "print (inverted_index1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Apache', 'doc1|doc5')\n",
      "('Spark', 'doc1')\n",
      "('Java', 'doc1')\n",
      "('C', 'doc1')\n",
      "('Python', 'doc1')\n",
      "('Do', 'doc1')\n",
      "('And', 'doc1')\n",
      "('Will', 'doc1')\n",
      "('News', 'doc2')\n",
      "('QQ', 'doc4')\n",
      "('Great', 'doc4')\n",
      "('MapReduce', 'doc5')\n",
      "('SVN', 'doc5')\n",
      "('Scala', 'doc1|doc2')\n",
      "('Hadoop', 'doc1|doc5')\n",
      "('KNN', 'doc1|doc3')\n",
      "('SVM', 'doc2|doc3|doc5')\n",
      "('Play', 'doc2')\n",
      "('Akka', 'doc2')\n",
      "('Yes', 'doc2')\n",
      "('GBDT', 'doc2|doc3')\n",
      "('LDA', 'doc3|doc4')\n",
      "('RF', 'doc3')\n",
      "('Adaboost', 'doc3')\n",
      "('Kmeans', 'doc3')\n",
      "('BAT', 'doc4')\n",
      "('I', 'doc4')\n",
      "('All', 'doc4')\n",
      "('Git', 'doc5')\n"
     ]
    }
   ],
   "source": [
    "for index in inverted_index1.collect() :\n",
    "    print (index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
